# 🧠 Deep Learning 핵심 개념 요약

본 문서는 데이터가 신경망을 통과하여 학습되는 전체 과정을 8가지 핵심 키워드로 정리합니다.

---

### 1️⃣ 모델의 구조 (Architecture)
데이터를 받아들이고 변환하여 예측값을 산출하는 뼈대입니다.

* **신경망 (Neural Network)**: 다층으로 쌓인 퍼셉트론을 통해 복잡한 데이터를 학습하는 기본 구조입니다.
* **활성화 함수 (Activation Function)**: 선형 결합에 비선형성을 부여(예: ReLU, Sigmoid)하여 모델의 표현력을 높입니다.
* **BatchNorm (Batch Normalization)**: 각 층의 출력을 정규화하여 학습 안정성을 높이고 Gradient 문제를 해결합니다.



---

### 2️⃣ 손실 함수 (Loss Function)
모델의 정답과 실제값 사이의 오차를 측정하는 기준입니다.

* **MSE (Mean Squared Error)**: 예측값과 실제값의 거리의 제곱을 평균냅니다. 주로 **회귀(Regression)** 문제에 사용됩니다.
* **BCEWithLogits**: 내부적으로 Sigmoid를 포함하여 확률적 오차를 계산합니다. **이진 분류(Binary Classification)**에 최적화되어 있습니다.

---

### 3️⃣ 학습 매커니즘 (Optimization)
계산된 오차를 바탕으로 모델을 똑똑하게 만드는 엔진입니다.

* **역전파 (Backpropagation)**: 오차를 출력층에서 입력층 방향으로 전달하며 각 가중치의 기여도(기울기)를 계산합니다.
* **Grad 분포 (Gradient Distribution)**: 역전파 시 흐르는 기울기 값의 분포입니다. BatchNorm은 이 분포를 일정하게 유지하는 역할을 합니다.
* **경사하강법 (Gradient Descent)**: 계산된 기울기(Grad)의 반대 방향으로 가중치를 조금씩 수정하여 최적의 지점을 찾아갑니다.



---

### 🚀 전체 프로세스 요약 (Flow)

1.  **Forward**: 데이터를 **신경망**에 입력하고 **BatchNorm**과 **활성화 함수**를 거쳐 예측값을 냅니다.
2.  **Loss**: 문제 유형에 따라 **MSE** 또는 **BCEWithLogits**로 오차를 계산합니다.
3.  **Backward**: **역전파**를 통해 각 파라미터의 **Grad 분포**를 확인하고 기울기를 구합니다.
4.  **Update**: **경사하강법**을 이용해 오차가 최소가 되는 방향으로 가중치를 갱신합니다.

---

### 💡 핵심 요약 표

| 구분 | 키워드 | 한 줄 정의 |
| :--- | :--- | :--- |
| **모델** | NN, 활성화함수, BatchNorm | 예측 모델의 설계 및 안정화 |
| **평가** | MSE, BCEWithLogits | 문제 유형에 따른 오차 측정 |
| **학습** | 역전파, 경사하강법, Grad분포 | 오차를 줄이기 위한 가중치 최적화 |
